{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import momepy as mm\n",
    "import networkx as nx\n",
    "from shapely import Point, LineString, reverse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from pyproj import Transformer\n",
    "import datetime\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.ops import unary_union\n",
    "from pyproj import Transformer\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. consum de aigua agrupat per EDIFICI \n",
    "\n",
    "1.1 agrupem per EDIFICI\n",
    "\n",
    "- fem mitjana data_lectura\n",
    " \n",
    "- fem suma de CONSUM\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EDIFICI  CONSUM mean_lectura_ini mean_lectura_fi\n",
      "0      2.0      32       2020-09-10      2021-01-15\n",
      "1      3.0      43       2020-09-10      2021-01-15\n",
      "2      5.0      28       2020-09-10      2021-01-15\n",
      "3      6.0       0       2020-09-10      2021-01-15\n",
      "4      9.0     135       2020-09-10      2021-01-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruben\\AppData\\Local\\Temp\\ipykernel_11684\\3650283003.py:4: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
      "  consum['DATA LECTURA INI'] = pd.to_datetime(consum['DATA LECTURA INI'])\n",
      "C:\\Users\\ruben\\AppData\\Local\\Temp\\ipykernel_11684\\3650283003.py:5: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
      "  consum['DATALECTURA FI'] = pd.to_datetime(consum['DATALECTURA FI'])\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "consum = pd.read_csv('data/consum/consum.csv')\n",
    "# Convert date columns to datetime format\n",
    "consum['DATA LECTURA INI'] = pd.to_datetime(consum['DATA LECTURA INI'])\n",
    "consum['DATALECTURA FI'] = pd.to_datetime(consum['DATALECTURA FI'])\n",
    "\n",
    "# Compute the mean of the date columns for each building\n",
    "#mean_dates = consum.groupby('EDIFICI')['DATA LECTURA INI', 'DATALECTURA FI'].mean()\n",
    "mean_dates = consum.groupby('EDIFICI')[['DATA LECTURA INI', 'DATALECTURA FI']].mean()\n",
    "\n",
    "# Rename columns to more meaningful names\n",
    "mean_dates.columns = ['mean_lectura_ini', 'mean_lectura_fi']\n",
    "\n",
    "# Group the consumption data by building and compute the total consumption\n",
    "consum_edifici = consum.groupby('EDIFICI')['CONSUM'].sum().reset_index()\n",
    "\n",
    "# Merge the consumption and date dataframes on the 'EDIFICI' column\n",
    "consum_edifici = pd.merge(consum_edifici, mean_dates, on='EDIFICI')\n",
    "\n",
    "# Print the resulting dataframe\n",
    "print(consum_edifici.head())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 associate to UMAT with commun index CODI_EDIFI\n",
    "\n",
    "NOTE: that in edificis_umat one same CODI_EDIFI may have several ATRIBUT (strings) and geometry\n",
    "\n",
    " - in ATRIBUT  we put all the atributes togueter separated by , like Px+T,I+T\n",
    " \n",
    " - in geometry we calculate the superposition of all the POLYGON and print a single poligon\n",
    "\n",
    "1.3 trobem centroide del poligon, extraiem longitud i latitud\n",
    "\n",
    "1.4 calculem PERIODE de consum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONSUM</th>\n",
       "      <th>ATRIBUT</th>\n",
       "      <th>geometry</th>\n",
       "      <th>SHAPE_AREA</th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>mean_lectura_ini</th>\n",
       "      <th>mean_lectura_fi</th>\n",
       "      <th>centroid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>PERIODE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CODI_EDIFI</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>64</td>\n",
       "      <td>I,III</td>\n",
       "      <td>POLYGON ((485674.577 4647942.149, 485674.378 4...</td>\n",
       "      <td>165.512811</td>\n",
       "      <td>875,12574</td>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>2021-01-15</td>\n",
       "      <td>POINT (485664.665 4647949.379)</td>\n",
       "      <td>41.983416</td>\n",
       "      <td>2.826950</td>\n",
       "      <td>1.896826e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>172</td>\n",
       "      <td>-I+IV+T,-I+V,III,-I+IV</td>\n",
       "      <td>POLYGON ((485673.255 4647980.942, 485657.993 4...</td>\n",
       "      <td>233.360891</td>\n",
       "      <td>20918,17785,42952,7170</td>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>2021-01-15</td>\n",
       "      <td>POINT (485670.238 4647986.229)</td>\n",
       "      <td>41.983748</td>\n",
       "      <td>2.827016</td>\n",
       "      <td>1.896826e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>56</td>\n",
       "      <td>I+T,III</td>\n",
       "      <td>POLYGON ((485685.177 4648005.528, 485684.191 4...</td>\n",
       "      <td>126.013930</td>\n",
       "      <td>41315,35410</td>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>2021-01-15</td>\n",
       "      <td>POINT (485678.831 4648004.349)</td>\n",
       "      <td>41.983911</td>\n",
       "      <td>2.827119</td>\n",
       "      <td>1.896826e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>0</td>\n",
       "      <td>III</td>\n",
       "      <td>POLYGON ((485686.089 4648012.111, 485685.505 4...</td>\n",
       "      <td>64.673028</td>\n",
       "      <td>10889</td>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>2021-01-15</td>\n",
       "      <td>POINT (485679.188 4648011.024)</td>\n",
       "      <td>41.983971</td>\n",
       "      <td>2.827124</td>\n",
       "      <td>1.896826e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>810</td>\n",
       "      <td>IV,Px+III,IV,-I+IV,II+T,II</td>\n",
       "      <td>POLYGON ((485687.156 4648019.939, 485675.597 4...</td>\n",
       "      <td>197.435127</td>\n",
       "      <td>2831,9123,13538,28932,32373,50878</td>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>2021-01-15</td>\n",
       "      <td>POINT (485683.869 4648028.151)</td>\n",
       "      <td>41.984126</td>\n",
       "      <td>2.827180</td>\n",
       "      <td>1.896826e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            CONSUM                     ATRIBUT  \\\n",
       "CODI_EDIFI                                       \n",
       "2.0             64                       I,III   \n",
       "3.0            172      -I+IV+T,-I+V,III,-I+IV   \n",
       "5.0             56                     I+T,III   \n",
       "6.0              0                         III   \n",
       "9.0            810  IV,Px+III,IV,-I+IV,II+T,II   \n",
       "\n",
       "                                                     geometry  SHAPE_AREA  \\\n",
       "CODI_EDIFI                                                                  \n",
       "2.0         POLYGON ((485674.577 4647942.149, 485674.378 4...  165.512811   \n",
       "3.0         POLYGON ((485673.255 4647980.942, 485657.993 4...  233.360891   \n",
       "5.0         POLYGON ((485685.177 4648005.528, 485684.191 4...  126.013930   \n",
       "6.0         POLYGON ((485686.089 4648012.111, 485685.505 4...   64.673028   \n",
       "9.0         POLYGON ((485687.156 4648019.939, 485675.597 4...  197.435127   \n",
       "\n",
       "                                     OBJECTID mean_lectura_ini  \\\n",
       "CODI_EDIFI                                                       \n",
       "2.0                                 875,12574       2020-09-10   \n",
       "3.0                    20918,17785,42952,7170       2020-09-10   \n",
       "5.0                               41315,35410       2020-09-10   \n",
       "6.0                                     10889       2020-09-10   \n",
       "9.0         2831,9123,13538,28932,32373,50878       2020-09-10   \n",
       "\n",
       "           mean_lectura_fi                        centroid   latitude  \\\n",
       "CODI_EDIFI                                                              \n",
       "2.0             2021-01-15  POINT (485664.665 4647949.379)  41.983416   \n",
       "3.0             2021-01-15  POINT (485670.238 4647986.229)  41.983748   \n",
       "5.0             2021-01-15  POINT (485678.831 4648004.349)  41.983911   \n",
       "6.0             2021-01-15  POINT (485679.188 4648011.024)  41.983971   \n",
       "9.0             2021-01-15  POINT (485683.869 4648028.151)  41.984126   \n",
       "\n",
       "            longitude       PERIODE  \n",
       "CODI_EDIFI                           \n",
       "2.0          2.826950  1.896826e+09  \n",
       "3.0          2.827016  1.896826e+09  \n",
       "5.0          2.827119  1.896826e+09  \n",
       "6.0          2.827124  1.896826e+09  \n",
       "9.0          2.827180  1.896826e+09  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llegim els edificis del UMAT\n",
    "edificis_umat = gpd.read_file('data/consum/edificis/edificis.shp')\n",
    "\n",
    "# 1.2 Merge the data frames on the common index\n",
    "merged = pd.merge(consum_edifici, edificis_umat, left_on='EDIFICI', right_on='CODI_EDIFI')\n",
    "\n",
    "# Group the merged data by the common index, and combine the attributes into a single string\n",
    "grouped = merged.groupby('CODI_EDIFI').agg({\n",
    "    'CONSUM': 'sum',\n",
    "    'ATRIBUT': lambda x: ','.join(x),\n",
    "    'geometry': lambda x: unary_union(x),\n",
    "    'SHAPE_AREA':'sum',\n",
    "    'OBJECTID':lambda x: ','.join(str(i) for i in x),\n",
    "    'mean_lectura_ini': lambda x: pd.to_datetime(x).mean(),\n",
    "    'mean_lectura_fi': lambda x: pd.to_datetime(x).mean()\n",
    "})\n",
    "\n",
    "# Convert the grouped data back to a GeoDataFrame\n",
    "result = gpd.GeoDataFrame(grouped, geometry='geometry')\n",
    "\n",
    "# 1.3 Extract the centroid of each geometry\n",
    "result['centroid'] = result.geometry.centroid\n",
    "\n",
    "# Convert the coordinates of the centroid from EPSG:25831 to WGS84\n",
    "transformer = Transformer.from_crs('EPSG:25831', 'EPSG:4326')\n",
    "result['latitude'], result['longitude'] = transformer.transform(result.centroid.x, result.centroid.y)\n",
    "\n",
    "# 1.4 Calculate the time differences in seconds and store them in a new column called 'PERIODE'\n",
    "result['PERIODE'] = (result['mean_lectura_fi'] - result['mean_lectura_ini'].min()).dt.total_seconds()\n",
    "\n",
    "# save the result\n",
    "result.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5 importem els manholes i trobem long lat\n",
    "\n",
    "1.6 spatial join to find the closest manhole to each location\n",
    "\n",
    "1.7 creem el dwf sumant els consums associats a cada node, afegim atributs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruben\\AppData\\Local\\Temp\\ipykernel_11684\\4014815828.py:27: UserWarning: CRS mismatch between the CRS of left geometries and the CRS of right geometries.\n",
      "Use `to_crs()` to reproject one of the input geometries to match the CRS of the other.\n",
      "\n",
      "Left CRS: GEOGCS[\"Undefined geographic SRS\",DATUM[\"unknown\", ...\n",
      "Right CRS: None\n",
      "\n",
      "  joined = gpd.sjoin_nearest(manholes, locations, how='left')\n",
      "c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\geopandas\\array.py:344: UserWarning: Geometry is in a geographic CRS. Results from 'sjoin_nearest' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id_old</th>\n",
       "      <th>CONSUM</th>\n",
       "      <th>PERIODE</th>\n",
       "      <th>code</th>\n",
       "      <th>pat1</th>\n",
       "      <th>value</th>\n",
       "      <th>dwfscencario_id</th>\n",
       "      <th>node_id_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>3480</td>\n",
       "      <td>1.897776e+09</td>\n",
       "      <td>PR0005291</td>\n",
       "      <td>hora</td>\n",
       "      <td>1.466980</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10018</td>\n",
       "      <td>15639</td>\n",
       "      <td>1.899729e+09</td>\n",
       "      <td>PR0023125</td>\n",
       "      <td>hora</td>\n",
       "      <td>6.585783</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>539</td>\n",
       "      <td>1.905984e+09</td>\n",
       "      <td>PR0024234</td>\n",
       "      <td>hora</td>\n",
       "      <td>0.226235</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10020</td>\n",
       "      <td>15639</td>\n",
       "      <td>1.899729e+09</td>\n",
       "      <td>PR0023124</td>\n",
       "      <td>hora</td>\n",
       "      <td>6.585783</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10022</td>\n",
       "      <td>112</td>\n",
       "      <td>1.899590e+09</td>\n",
       "      <td>PR0023123</td>\n",
       "      <td>hora</td>\n",
       "      <td>0.047168</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  node_id_old  CONSUM       PERIODE       code  pat1     value  \\\n",
       "0          10    3480  1.897776e+09  PR0005291  hora  1.466980   \n",
       "1       10018   15639  1.899729e+09  PR0023125  hora  6.585783   \n",
       "2        1002     539  1.905984e+09  PR0024234  hora  0.226235   \n",
       "3       10020   15639  1.899729e+09  PR0023124  hora  6.585783   \n",
       "4       10022     112  1.899590e+09  PR0023123  hora  0.047168   \n",
       "\n",
       "   dwfscencario_id  node_id_new  \n",
       "0                2          NaN  \n",
       "1                2          NaN  \n",
       "2                2          NaN  \n",
       "3                2          NaN  \n",
       "4                2          NaN  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consum_geo=result\n",
    "\n",
    "# Convert the latitude and longitude columns of consum_geo to a shapely Point object\n",
    "geometry = [Point(long, lat) for lat, long in zip(consum_geo['latitude'], consum_geo['longitude'])]\n",
    "locations = gpd.GeoDataFrame(consum_geo, geometry=geometry)\n",
    "\n",
    "#1.5 importem els manholes\n",
    "nodes_corregits = gpd.read_file(\"results/networks/nodes_corretgits.gpkg\")\n",
    "manholes = nodes_corregits[(nodes_corregits['sys_type']=='MANHOLE') & (nodes_corregits['state'] != 0)]   # Filter by MANHOLE sys_type\n",
    "\n",
    "#transformem les coordenades dels manholes a long, lat\n",
    "transformer = Transformer.from_crs(\"EPSG:25831\", \"WGS84\")\n",
    "for index, row in manholes.iterrows():\n",
    "     #if row['state'] != 0:\n",
    "         x = row['geometry'].x\n",
    "         y = row['geometry'].y\n",
    "\n",
    "         long, lat = transformer.transform(x, y)\n",
    "         row['longitude'] = long\n",
    "         row['latitude'] = lat\n",
    "         row['x'] = x\n",
    "         row['y'] = y\n",
    "        \n",
    "         row['geometry'] = (long, lat)\n",
    "\n",
    "# 1.6 Perform a spatial join to find the closest manhole to each location\n",
    "joined = gpd.sjoin_nearest(manholes, locations, how='left')\n",
    "\n",
    "\n",
    "# Calculate the distance between each location and its closest manhole\n",
    "#joined['distance'] = joined.apply(lambda x: distance((x['latitude'], x['longitude']), (x['geometry'].y, x['geometry'].x)).meters, axis=1)\n",
    "\n",
    "#1.7 creem el dwf\n",
    "manholes_dwf = joined.groupby('node_id').agg({\n",
    "    'CONSUM': 'sum',\n",
    "    'PERIODE': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Add the manhole code to the manholes_dwf DataFrame\n",
    "manholes_dwf = manholes_dwf.merge(manholes[['node_id', 'code']], on='node_id', how='left')\n",
    "\n",
    "# Add the columns pat1, value, dscencario, and new_id with NaN values\n",
    "\n",
    "cm3=1\n",
    "m3 = 1000000*cm3\n",
    "\n",
    "manholes_dwf['pat1'] = 'hora'\n",
    "manholes_dwf['value'] = 0.8 * manholes_dwf['CONSUM'] * m3 / manholes_dwf['PERIODE']\n",
    "manholes_dwf['dwfscencario_id'] =2\n",
    "manholes_dwf['node_id_new'] = float('nan')\n",
    "\n",
    "\n",
    "manholes_dwf = manholes_dwf.rename(columns={'node_id': 'node_id_old'})\n",
    "manholes_dwf.to_csv('data/consum/manholes_dwf.csv', index=False)\n",
    "\n",
    "manholes_dwf.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use the diccionary to fill the node_id with the new giswater index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>pat1</th>\n",
       "      <th>value</th>\n",
       "      <th>dwfscencario_id</th>\n",
       "      <th>node_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PR0005291</td>\n",
       "      <td>hora</td>\n",
       "      <td>1.466980</td>\n",
       "      <td>2</td>\n",
       "      <td>13026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PR0023125</td>\n",
       "      <td>hora</td>\n",
       "      <td>6.585783</td>\n",
       "      <td>2</td>\n",
       "      <td>14499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PR0024234</td>\n",
       "      <td>hora</td>\n",
       "      <td>0.226235</td>\n",
       "      <td>2</td>\n",
       "      <td>13148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PR0023124</td>\n",
       "      <td>hora</td>\n",
       "      <td>6.585783</td>\n",
       "      <td>2</td>\n",
       "      <td>14500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PR0023123</td>\n",
       "      <td>hora</td>\n",
       "      <td>0.047168</td>\n",
       "      <td>2</td>\n",
       "      <td>14501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        code  pat1     value  dwfscencario_id  node_id\n",
       "0  PR0005291  hora  1.466980                2    13026\n",
       "1  PR0023125  hora  6.585783                2    14499\n",
       "2  PR0024234  hora  0.226235                2    13148\n",
       "3  PR0023124  hora  6.585783                2    14500\n",
       "4  PR0023123  hora  0.047168                2    14501"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "manholes_dwf=pd.read_csv('data/consum/manholes_dwf.csv')\n",
    "dic_node_id = pd.read_csv(\"data/networks/dic_node_id.csv\")\n",
    "\n",
    "\n",
    "# Merge the DataFrames on the 'code' column\n",
    "manholes_dwf = pd.merge(manholes_dwf, dic_node_id[['code', 'node_id_new']], on='code', how='left').head()\n",
    "\n",
    "\n",
    "manholes_dwf = manholes_dwf.rename(columns={'node_id_new_y': 'node_id'})\n",
    "\n",
    "# drop unecessary columns\n",
    "manholes_dwf = manholes_dwf.drop(columns=['node_id_new_x','node_id_old','CONSUM', 'PERIODE'])\n",
    "\n",
    "\n",
    "# Save the manholes_dwf DataFrame to a CSV file\n",
    "inp_dwf=manholes_dwf\n",
    "inp_dwf.to_csv('data/consum/inp_dwf.csv', index=False)\n",
    "\n",
    "inp_dwf.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Consum de aigua geolocalitzat per adressa\n",
    "\n",
    "1. agrupem els contadors amb igual ('NOM CARRER') i ('NUM')\n",
    "\n",
    " - sumem els consums i calculem temps de factura en segons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv file into a Pandas dataframe\n",
    "consum = pd.read_csv('data/consum/consum.csv')\n",
    "\n",
    "print(consum.columns)\n",
    "\n",
    "#dwf = consum[['NOM CARRER', 'NUM','EDIFICI', 'CONSUM','DATA LECTURA INI','DATALECTURA FI']]\n",
    "\n",
    "# Convert the 'DATA LECTURA INI' column to a datetime format\n",
    "consum['DATA LECTURA INI'] = pd.to_datetime(consum['DATA LECTURA INI'],dayfirst=True)\n",
    "consum['DATALECTURA FI'] = pd.to_datetime(consum['DATALECTURA FI'], dayfirst=True)\n",
    "\n",
    "# Group the rows by 'NOM CARRER' and 'NUM', and calculate the sum of 'CONSUM' for each group\n",
    "consum_grouped = consum.groupby(['NOM CARRER', 'NUM']).agg({'CONSUM': 'sum'})\n",
    "\n",
    "# Calculate the mean datetime values for 'DATA LECTURA INI' and 'DATALECTURA FI' for each group\n",
    "consum_grouped['DATA LECTURA INI'] = consum.groupby(['NOM CARRER', 'NUM'])['DATA LECTURA INI'].mean()\n",
    "consum_grouped['DATALECTURA FI'] = consum.groupby(['NOM CARRER', 'NUM'])['DATALECTURA FI'].mean()\n",
    "\n",
    "# Reset the index to make the group keys ('NOM CARRER' and 'NUM') columns again\n",
    "consum_grouped = consum_grouped.reset_index()\n",
    "\n",
    "# Calculate the time differences in seconds and store them in a new column called 'PERIODE'\n",
    "consum_grouped['PERIODE'] = (consum_grouped['DATALECTURA FI'] - consum_grouped['DATA LECTURA INI'].min()).dt.total_seconds()\n",
    "consum_grouped.to_csv('data/consum/consum_grouped.csv', index=False)\n",
    "consum_grouped"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "determine their latitude and longitude useing the Geopy library.\n",
    "\n",
    "- separem les entrades per grups de 200 perque el porgrama no reventi\n",
    "\n",
    "- we add try and esceptions for lost NUM lost NOM CARRER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "import requests\n",
    "\n",
    "#TODO: i keep seeing the warning in jupiter notebook enevn i try to stop it\n",
    "# what shall i do? i can send u the warning if u ask\n",
    "# Suppress the 'SettingWithCopyWarning' warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=Warning, module='pandas')\n",
    "\n",
    "# Define the filename of your input CSV file\n",
    "input_file = 'data/consum/consum_grouped.csv'\n",
    "\n",
    "# Define the filename of your output CSV file\n",
    "output_file = 'data/consum/consum_grouped_located.csv'\n",
    "\n",
    "# Define the chunksize (number of rows to process at a time)\n",
    "chunksize = 50\n",
    "\n",
    "# compute the total number of chunks\n",
    "total_chunks = pd.read_csv(input_file, nrows=1).shape[0]\n",
    "print('Please be patient, we will geolocalize addresses: ')\n",
    "print('Total ( ',total_chunks//chunksize+1,' ) chunks with len = (',chunksize,')' )\n",
    "\n",
    "\n",
    "# Create a geolocator object\n",
    "geolocator = Nominatim(user_agent='my-app',timeout=50)\n",
    "\n",
    "# Define a function to get the latitude and longitude of an address\n",
    "# we try diferent convinations to find missing adresses\n",
    "\n",
    "\n",
    "def get_lat_long(row):\n",
    "    address = f\"carrer {row['NOM CARRER']}, {row['NUM']}, Gerona, Spain\"\n",
    "    location = geolocator.geocode(address)\n",
    "    if location:\n",
    "        return (location.latitude, location.longitude)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_lat_long_dif(row):\n",
    "    # Split the street name into separate words\n",
    "    words = row['NOM CARRER'].split()\n",
    "    \n",
    "    # Create a list of all possible combinations of the words, with one word removed in each combination\n",
    "    combinations = []\n",
    "    for i in range(len(words)):\n",
    "        combination = ' '.join(words[:i] + words[i+1:])\n",
    "        combinations.append(combination)\n",
    "    \n",
    "    # Try geocoding each combination\n",
    "    for combination in combinations:\n",
    "        address = f\"carrer {combination}, {row['NUM']}, Gerona, Spain\"\n",
    "        location = geolocator.geocode(address)\n",
    "        if location:\n",
    "            return (location.latitude, location.longitude)\n",
    "    \n",
    "    # If none of the combinations work, return None\n",
    "    return None\n",
    "    \n",
    "def get_lat_long_difSN(row):\n",
    "    # Split the street name into separate words\n",
    "    words = row['NOM CARRER'].split()\n",
    "    \n",
    "    # Create a list of all possible combinations of the words, with one word removed in each combination\n",
    "    combinations = []\n",
    "    for i in range(len(words)):\n",
    "        combination = ' '.join(words[:i] + words[i+1:])\n",
    "        combinations.append(combination)\n",
    "    \n",
    "    # Try geocoding each combination\n",
    "    for combination in combinations:\n",
    "        address = f\"carrer {combination}, Gerona, Spain\"\n",
    "        location = geolocator.geocode(address)\n",
    "        if location:\n",
    "            return (location.latitude, location.longitude)\n",
    "    \n",
    "    # If none of the combinations work, return None\n",
    "    return None\n",
    "\n",
    "def get_lat_long_SN(row):\n",
    "    # Remove NUM from the address\n",
    "    address = f\"carrer {row['NOM CARRER']}, Gerona, Spain\"\n",
    "    location = geolocator.geocode(address)\n",
    "    if location:\n",
    "        return (location.latitude, location.longitude)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Define a function to process a chunk of the DataFrame and write the results to a CSV file\n",
    "def process_chunk(chunk, i, max_retries=20):\n",
    "    # Apply the function to the dataframe to get the latitude and longitude of each address\n",
    "    for retry in range(max_retries):\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            chunk['lat_long'] = chunk.apply(get_lat_long, axis=1)\n",
    "            end_time = time.time()\n",
    "\n",
    "            # Print the time taken to geocode the addresses in this chunk\n",
    "            print(f\"Time taken for the {i}/{total_chunks//chunksize+1} geocode with ( {len(chunk)} ) addresses: {end_time - start_time} seconds\")\n",
    "\n",
    "    \n",
    "            # recompute the values with none lat_long without NUM\n",
    "            if chunk['lat_long'].isnull().any():\n",
    "            # Recompute the latitude and longitude values without NUM\n",
    "                without_num = chunk[chunk['lat_long'].isnull()]\n",
    "                print('!  full adresses not localized', len(without_num))\n",
    "\n",
    "\n",
    "                if len(without_num) > 0:\n",
    "                    without_num.loc[:, 'lat_long'] = without_num.apply(get_lat_long_dif, axis=1)\n",
    "                    chunk.loc[without_num.index, 'lat_long'] = without_num['lat_long']\n",
    "                    without_num = chunk[chunk['lat_long'].isnull()]\n",
    "                    print('!!  dif adress not localized', len(without_num))\n",
    "\n",
    "                if len(without_num) > 0:\n",
    "                    without_num.loc[:, 'lat_long'] = without_num.apply(get_lat_long_SN, axis=1)\n",
    "                    chunk.loc[without_num.index, 'lat_long'] = without_num['lat_long']\n",
    "                    without_num = chunk[chunk['lat_long'].isnull()]\n",
    "                    print('!!!  SN adress not localized', len(without_num))\n",
    "\n",
    "                if len(without_num) > 0:\n",
    "                    without_num.loc[:, 'lat_long'] = without_num.apply(get_lat_long_difSN, axis=1)\n",
    "                    chunk.loc[without_num.index, 'lat_long'] = without_num['lat_long']\n",
    "                    without_num = chunk[chunk['lat_long'].isnull()]\n",
    "                    print('!!!!  dif+SN adress not localized', len(without_num))\n",
    "                \n",
    "                if len(without_num) == 0:\n",
    "                    print('-> all adresses have been localized')\n",
    "\n",
    "                if len(without_num) != 0:\n",
    "                    print('ERROR  (',len(without_num),')  adress not localized')\n",
    "\n",
    "\n",
    "            # Extract the latitude and longitude into separate columns\n",
    "            chunk['latitude'] = chunk['lat_long'].apply(lambda x: x[0] if x else None)\n",
    "            chunk['longitude'] = chunk['lat_long'].apply(lambda x: x[1] if x else None)\n",
    "            # Save the chunk as a new .csv file\n",
    "            if i == 1:\n",
    "                chunk.to_csv(output_file, mode='w', index=False)\n",
    "            else:\n",
    "                chunk.to_csv(output_file, mode='a', index=False, header=False)\n",
    "\n",
    "            \n",
    "            # Exit the retry loop if the chunk was processed successfully\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing chunk ( {i} ), retrying ({retry+1}/{max_retries})...\")\n",
    "            print(f\"Error message: {e}\")\n",
    "    \n",
    "    # Raise an exception if the maximum number of retries was reached without success\n",
    "    else:\n",
    "     #raise Exception(f\"Failed to process chunk ( {i} ) after {max_retries} retries\")\n",
    "     #TODO is the following a good alternative?, i want to save none values in longitude, latitude, lat_long\n",
    "     # in order to fill if unexistent(?) fields and continue for the next chunk \n",
    "\n",
    "\n",
    "        print(f\"Failed to process chunk ( {i} ) after {max_retries} retries\")\n",
    "        # Extract the latitude and longitude into separate columns\n",
    "        #chunk['lat_long'] = [(None,None)] * len(chunk)\n",
    "        chunk['latitude'] = chunk['lat_long'].apply(lambda x: x[0] if x else None)\n",
    "        chunk['longitude'] = chunk['lat_long'].apply(lambda x: x[1] if x else None)\n",
    "        #TODO not sure if last 3 lines are necessary \n",
    "        # Save the chunk as a new .csv file\n",
    "        if i == 1:\n",
    "            chunk.to_csv(output_file, mode='w', index=False)\n",
    "        else:\n",
    "            chunk.to_csv(output_file, mode='a', index=False, header=False)\n",
    "        \n",
    "        #raise Exception(f\"Failed to process chunk ( {i} ) after {max_retries} retries\")\n",
    "\n",
    "# Process the DataFrame in chunks of chunksize rows\n",
    "\n",
    "#TODO if the values at 'latitude' of the output file (in case that it exisit) are all real fload jump to the next chunk \n",
    "i = 1\n",
    "for chunk in pd.read_csv(input_file, chunksize=chunksize):\n",
    "    process_chunk(chunk, i)\n",
    "    i += 1\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "veiem les adresses conflitives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read in the data\n",
    "data = pd.read_csv('data/consum/consum_grouped_located.csv')\n",
    "\n",
    "# Find all the NaN values in the 'latitude' column\n",
    "nan_rows = data[data['latitude'].isna()]\n",
    "\n",
    "print(len(nan_rows))\n",
    "# Loop through each NaN row and display the corresponding street and number\n",
    "#for _, row in nan_rows.iterrows():\n",
    "#    print(f\"Customize NaN value for: {row['NOM CARRER']}, {row['NUM']}\")\n",
    "    # try to geolocalize the longitude and latitude of thois street by using some python librtary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_lat_long_API(address):\n",
    "    url = 'https://nominatim.openstreetmap.org/search'\n",
    "    #url = 'https://maps.googleapis.com/maps/api/geocode/json'\n",
    "    #url = 'https://dev.virtualearth.net/REST/v1/Locations'\n",
    "\n",
    "\n",
    "    params = {\n",
    "        'q': address,\n",
    "        'format': 'json'\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    if len(data) > 0:\n",
    "        lat = data[0]['lat']\n",
    "        lon = data[0]['lon']\n",
    "        return float(lat), float(lon)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#address = '1600 Amphitheatre Parkway, Mountain View, CA'\n",
    "address = '7 , AVDA. RAMON FOLCH, Girona, Spain'\n",
    "result = get_lat_long_API(address)\n",
    "if result is not None:\n",
    "    lat, lon = result\n",
    "    print(f'The latitude and longitude of {address} are: {lat}, {lon}')\n",
    "else:\n",
    "    print(f'Could not find the latitude and longitude of {address}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Read in the data\n",
    "data = pd.read_csv('data/consum/consum_grouped_located.csv')\n",
    "\n",
    "# Find all the NaN values in the 'latitude' column\n",
    "nan_rows = data[data['latitude'].isna()]\n",
    "\n",
    "# Set up a geolocator object using Nominatim\n",
    "geolocator = Nominatim(user_agent='my-app')\n",
    "\n",
    "# Loop through each NaN row and display the corresponding street and number\n",
    "for _, row in nan_rows.iterrows():\n",
    "    # Get the address string for the street and number\n",
    "    address = f\"{row['NOM CARRER']}, Girona, Spain\"\n",
    "    # Use geopy to geolocalize the address\n",
    "    \n",
    "    try: \n",
    "        lat, lon = get_lat_long_API(address)\n",
    "        \n",
    "        if type(lat)!=float:  #TODO we may try another appy here instead\n",
    "            location = geolocator.geocode(address)\n",
    "            lat=location.latitude\n",
    "            loc=location.longitude\n",
    "    except:\n",
    "        lat=None\n",
    "        lon=None\n",
    "\n",
    "\n",
    "    #TODO we may try another api here instead\n",
    "    try: \n",
    "        location = geolocator.geocode(address)\n",
    "        lat=location.latitude\n",
    "        loc=location.longitude\n",
    "    except:\n",
    "        lat=None\n",
    "        lon=None\n",
    "\n",
    "\n",
    "\n",
    "    if type(lat)==float:\n",
    "        print(lat,'   ',lon)\n",
    "        data.loc[(data['NUM']==row['NUM']) & (data['NOM CARRER']==row['NOM CARRER']), ['latitude', 'longitude']] = [lat, lon]\n",
    "        #TODO: if found, update the values in data['latitude'], data['longitude'] from my file .csv\n",
    "    else:\n",
    "        print(f\"No location found for: {address}\")\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "data.to_csv('data/consum/consum_grouped_located_updated.csv', index=False)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "associem el consum de cada edifici al manhole mes proper del nostre subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "# from geopy.distance import distance\n",
    "# from geopandas.tools import nearest_neighbor\n",
    "\n",
    "# Load the csv file into a Pandas dataframe\n",
    "dwf0 =  pd.read_csv('data/consum/consum_grouped_located_updated.csv')\n",
    "# Find all the not NaN values in the 'latitude' column\n",
    "dwf = dwf0[dwf0['latitude'].notna()]\n",
    "\n",
    "node_corregit = gpd.read_file(\"results/networks/nodes_corretgits.gpkg\")\n",
    "manholes = node_corregit[(node_corregit['sys_type']=='MANHOLE') & (node_corregit['state'] != 0)]   # Filter by MANHOLE sys_type\n",
    "\n",
    "# Convert the latitude and longitude columns of dwf to a shapely Point object\n",
    "geometry = [Point(long, lat) for lat, long in zip(dwf['latitude'], dwf['longitude'])]\n",
    "locations = gpd.GeoDataFrame(dwf, geometry=geometry)\n",
    "\n",
    "#transformem les coordenades\n",
    "transformer = Transformer.from_crs(\"EPSG:25831\", \"WGS84\")\n",
    "\n",
    "#afegim els nodes\n",
    "for index, row in manholes.iterrows():\n",
    "     #if row['state'] != 0:\n",
    "         x = row['geometry'].x\n",
    "         y = row['geometry'].y\n",
    "\n",
    "         long, lat = transformer.transform(x, y)\n",
    "         row['longitude'] = long\n",
    "         row['latitude'] = lat\n",
    "         row['x'] = x\n",
    "         row['y'] = y\n",
    "        \n",
    "         row['geometry'] = (long, lat)\n",
    "\n",
    "# Perform a spatial join to find the closest manhole to each location\n",
    "joined = gpd.sjoin_nearest(manholes, locations, how='left')\n",
    "\n",
    "\n",
    "# Calculate the distance between each location and its closest manhole\n",
    "#joined['distance'] = joined.apply(lambda x: distance((x['latitude'], x['longitude']), (x['geometry'].y, x['geometry'].x)).meters, axis=1)\n",
    "\n",
    "# create a new df manholes_dwf where we add all the 'value' form my dwf['value'] with same manhole_id, \n",
    "# we should get a list of manholes with sum_value atribute with the sum of all the closest dwf\n",
    "manholes_dwf = joined.groupby('node_id').agg({'CONSUM': 'sum'}).reset_index()\n",
    "\n",
    "#to add the code\n",
    "manholes_dwf = manholes_dwf.merge(manholes[['node_id', 'code']], on='node_id', how='left')\n",
    "\n",
    "\n",
    "# Save the joined dataframe to a new csv file\n",
    "manholes_dwf.to_csv('data/manholes_dwf.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
